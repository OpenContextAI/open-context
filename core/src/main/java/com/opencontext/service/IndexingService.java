package com.opencontext.service;

import com.opencontext.dto.StructuredChunk;
import com.opencontext.entity.DocumentChunk;
import com.opencontext.entity.SourceDocument;
import com.opencontext.enums.ErrorCode;
import com.opencontext.exception.BusinessException;
import com.opencontext.repository.DocumentChunkRepository;
import com.opencontext.repository.SourceDocumentRepository;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.*;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;
import org.springframework.web.client.RestTemplate;

import java.nio.charset.StandardCharsets;
import java.util.*;
import java.util.Arrays;

/**
 * ì„ë² ë”©ëœ ì²­í¬ë¥¼ Elasticsearchì™€ PostgreSQLì— ì €ì¥í•˜ëŠ” ì„œë¹„ìŠ¤.
 * 
 * Elasticsearchì—ëŠ” ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼,
 * Service for storing embedded chunks in Elasticsearch and PostgreSQL.
 * 
 * Stores vectors and metadata in Elasticsearch for search,
 * and hierarchical structure information in PostgreSQL.
 */
@Slf4j
@Service
@RequiredArgsConstructor
@Transactional
public class IndexingService {

    private final DocumentChunkRepository documentChunkRepository;
    private final SourceDocumentRepository sourceDocumentRepository;
    private final RestTemplate restTemplate;
    private final ObjectMapper objectMapper;

    @Value("${app.elasticsearch.url:http://localhost:9200}")
    private String elasticsearchUrl;

    @Value("${app.elasticsearch.index:document-chunks}")
    private String indexName;

    /**
     * ì„ë² ë”©ëœ ì²­í¬ë“¤ì„ Elasticsearchì™€ PostgreSQLì— ì €ì¥í•©ë‹ˆë‹¤.
     * 
     * @param documentId ë¬¸ì„œ ID
     * @param embeddedChunks ì €ì¥í•  ì„ë² ë”©ëœ ì²­í¬ ëª©ë¡
     */
    public void indexChunks(UUID documentId, List<StructuredChunk> embeddedChunks) {
        long startTime = System.currentTimeMillis();
        int totalChunks = embeddedChunks.size();
        
        log.info("ğŸ“ [INDEXING] Starting chunk indexing: documentId={}, chunks={}", documentId, totalChunks);

        try {
            // Step 1: Elasticsearchì— ë²¡í„° ë°ì´í„° ì €ì¥
            log.debug("ğŸ” [INDEXING] Step 1/2: Indexing to Elasticsearch: chunks={}", totalChunks);
            bulkIndexToElasticsearch(embeddedChunks);
            log.info("âœ… [INDEXING] Elasticsearch indexing completed: documentId={}, chunks={}", documentId, totalChunks);
            
            // Step 2: PostgreSQLì— ê³„ì¸µ êµ¬ì¡° ì •ë³´ ì €ì¥
            log.debug("ğŸ’¾ [INDEXING] Step 2/2: Saving hierarchy to PostgreSQL: chunks={}", totalChunks);
            int savedChunks = saveChunkHierarchyToPostgreSQL(documentId, embeddedChunks);
            log.info("âœ… [INDEXING] PostgreSQL hierarchy saved: documentId={}, savedChunks={}", documentId, savedChunks);
            
            long duration = System.currentTimeMillis() - startTime;
            log.info("ğŸ‰ [INDEXING] Chunk indexing completed successfully: documentId={}, chunks={}, duration={}ms", 
                    documentId, totalChunks, duration);

        } catch (Exception e) {
            long duration = System.currentTimeMillis() - startTime;
            log.error("âŒ [INDEXING] Chunk indexing failed: documentId={}, chunks={}, duration={}ms, error={}", 
                    documentId, totalChunks, duration, e.getMessage(), e);
            throw new BusinessException(ErrorCode.INDEXING_FAILED, 
                    "Failed to index chunks: " + e.getMessage());
        }
    }

    /**
     * Elasticsearchì— ì²­í¬ë“¤ì„ ë²Œí¬ ì¸ë±ì‹±í•©ë‹ˆë‹¤.
     */
    private void bulkIndexToElasticsearch(List<StructuredChunk> chunks) {
        long esStartTime = System.currentTimeMillis();
        int totalChunks = chunks.size();
        
        log.debug("ğŸ” [ELASTICSEARCH] Starting bulk indexing: chunks={}, index={}", totalChunks, indexName);

        try {
            // ë²Œí¬ ìš”ì²­ ë°”ë”” ìƒì„±
            log.debug("ğŸ“¦ [ELASTICSEARCH] Building bulk request body: chunks={}", totalChunks);
            StringBuilder bulkBody = new StringBuilder();
            
            for (StructuredChunk chunk : chunks) {
                // ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„°
                Map<String, Object> indexMeta = Map.of(
                        "index", Map.of("_id", chunk.getChunkId())
                );
                try {
                    bulkBody.append(objectMapper.writeValueAsString(indexMeta)).append("\n");
                    
                    // ë¬¸ì„œ ë°ì´í„° (PRD ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜)
                    Map<String, Object> doc = createElasticsearchDocumentPRD(chunk);
                    bulkBody.append(objectMapper.writeValueAsString(doc)).append("\n");
                } catch (Exception jsonException) {
                    log.error("JSON ì§ë ¬í™” ì‹¤íŒ¨, ì²­í¬ ê±´ë„ˆë›°ê¸°: chunkId={}, error={}", 
                            chunk.getChunkId(), jsonException.getMessage());
                    continue; // í•´ë‹¹ ì²­í¬ëŠ” ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
                }
            }

            // HTTP í—¤ë” ì„¤ì • (UTF-8 ëª…ì‹œ)
            HttpHeaders headers = new HttpHeaders();
            headers.setContentType(new MediaType("application", "x-ndjson", StandardCharsets.UTF_8));

            // ë³¸ë¬¸ì„ UTF-8 ë°”ì´íŠ¸ë¡œ ì „ì†¡í•˜ì—¬ í•œê¸€ì´ '?'ë¡œ ì¹˜í™˜ë˜ëŠ” ë¬¸ì œ ë°©ì§€
            byte[] requestBytes = bulkBody.toString().getBytes(StandardCharsets.UTF_8);
            HttpEntity<byte[]> requestEntity = new HttpEntity<>(requestBytes, headers);

            // ë²Œí¬ API í˜¸ì¶œ
            ResponseEntity<Map<String, Object>> response = restTemplate.exchange(
                    elasticsearchUrl + "/" + indexName + "/_bulk",
                    HttpMethod.POST,
                    requestEntity,
                    (Class<Map<String, Object>>) (Class<?>) Map.class
            );

            if (response.getStatusCode() != HttpStatus.OK) {
                throw new BusinessException(ErrorCode.EXTERNAL_API_ERROR, 
                        "Elasticsearch bulk indexing failed");
            }

            // ë²Œí¬ ì‘ë‹µì—ì„œ ì˜¤ë¥˜ í™•ì¸ (PRD ìš”êµ¬ì‚¬í•­)
            Map<String, Object> responseBody = response.getBody();
            if (responseBody != null && Boolean.TRUE.equals(responseBody.get("errors"))) {
                // ì²« ë²ˆì§¸ ì—ëŸ¬ì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                List<Map<String, Object>> items = (List<Map<String, Object>>) responseBody.get("items");
                if (items != null && !items.isEmpty()) {
                    Map<String, Object> firstItem = items.get(0);
                    Map<String, Object> indexResult = (Map<String, Object>) firstItem.get("index");
                    if (indexResult != null && indexResult.containsKey("error")) {
                        Map<String, Object> error = (Map<String, Object>) indexResult.get("error");
                        String reason = (String) error.get("reason");
                        log.error("Elasticsearch bulk indexing error: {}", reason);
                        throw new BusinessException(ErrorCode.EXTERNAL_API_ERROR, 
                                "Elasticsearch bulk indexing failed: " + reason);
                    }
                }
                throw new BusinessException(ErrorCode.EXTERNAL_API_ERROR, 
                        "Elasticsearch bulk indexing failed with errors");
            }

            long totalDuration = System.currentTimeMillis() - esStartTime;
            log.info("âœ… [ELASTICSEARCH] Bulk indexing completed successfully: chunks={}, duration={}ms", 
                    totalChunks, totalDuration);

        } catch (Exception e) {
            long totalDuration = System.currentTimeMillis() - esStartTime;
            log.error("âŒ [ELASTICSEARCH] Bulk indexing failed: chunks={}, duration={}ms, error={}", 
                    totalChunks, totalDuration, e.getMessage(), e);
            throw new BusinessException(ErrorCode.EXTERNAL_API_ERROR, 
                    "Failed to index to Elasticsearch: " + e.getMessage());
        }
    }

    /**
     * PostgreSQLì— ì²­í¬ ê³„ì¸µ êµ¬ì¡° ì •ë³´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
     */
    private int saveChunkHierarchyToPostgreSQL(UUID documentId, List<StructuredChunk> chunks) {
        long pgStartTime = System.currentTimeMillis();
        int totalChunks = chunks.size();
        
        log.debug("ğŸ’¾ [POSTGRESQL] Starting to save chunk hierarchy: documentId={}, chunks={}", 
                documentId, totalChunks);

        try {
            // âœ… SourceDocument ì—”í‹°í‹°ë¥¼ ê°€ì ¸ì™€ì„œ JPA ê´€ê³„ ì„¤ì •
            SourceDocument sourceDocument = sourceDocumentRepository.findById(documentId)
                    .orElseThrow(() -> new BusinessException(ErrorCode.DATABASE_ERROR, 
                            "SourceDocument not found: " + documentId));

            List<DocumentChunk> documentChunks = new ArrayList<>();

            for (StructuredChunk chunk : chunks) {
                UUID parentChunkUuid = null;
                if (chunk.getParentChunkId() != null && !chunk.getParentChunkId().trim().isEmpty()) {
                    try {
                        parentChunkUuid = UUID.fromString(chunk.getParentChunkId());
                    } catch (IllegalArgumentException e) {
                        log.warn("Invalid parent_chunk_id format: {}, skipping parent relationship for chunk: {}", 
                                chunk.getParentChunkId(), chunk.getChunkId());
                        parentChunkUuid = null;
                    }
                }

                DocumentChunk documentChunk = DocumentChunk.builder()
                        .chunkId(chunk.getChunkId())
                        .sourceDocument(sourceDocument)
                        .sourceDocumentId(documentId)
                        .title(chunk.getTitle())
                        .hierarchyLevel(chunk.getHierarchyLevel())
                        .parentChunkId(parentChunkUuid)
                        .elementType(chunk.getElementType())
                        .sequenceInDocument(0) // ê¸°ë³¸ê°’ ì„¤ì •, ì‹¤ì œë¡œëŠ” ìˆœì„œì— ë”°ë¼ ì„¤ì •í•´ì•¼ í•¨
                        .build();

                documentChunks.add(documentChunk);
            }

            // ë°°ì¹˜ ì €ì¥
            long saveStartTime = System.currentTimeMillis();
            List<DocumentChunk> savedChunks = documentChunkRepository.saveAll(documentChunks);
            long saveDuration = System.currentTimeMillis() - saveStartTime;
            
            long totalDuration = System.currentTimeMillis() - pgStartTime;
            log.info("âœ… [POSTGRESQL] Chunk hierarchy saved successfully: documentId={}, chunks={}, duration={}ms, saveTime={}ms", 
                    documentId, savedChunks.size(), totalDuration, saveDuration);
            
            return savedChunks.size();

        } catch (Exception e) {
            long totalDuration = System.currentTimeMillis() - pgStartTime;
            log.error("âŒ [POSTGRESQL] Failed to save chunk hierarchy: documentId={}, chunks={}, duration={}ms, error={}", 
                    documentId, totalChunks, totalDuration, e.getMessage(), e);
            throw new BusinessException(ErrorCode.DATABASE_ERROR, 
                    "Failed to save chunk hierarchy: " + e.getMessage());
        }
    }

    /**
     * PRD ìŠ¤í‚¤ë§ˆì— ë§ëŠ” Elasticsearch ë¬¸ì„œ ìƒì„±
     * í•„ë“œëª…: camelCase, metadata í•˜ìœ„êµ¬ì¡° ì¤€ìˆ˜
     */
    private Map<String, Object> createElasticsearchDocumentPRD(StructuredChunk chunk) {
        Map<String, Object> doc = new HashMap<>();
        
        // PRD ë£¨íŠ¸ í•„ë“œ (camelCase)
        doc.put("chunkId", chunk.getChunkId());
        doc.put("sourceDocumentId", chunk.getDocumentId());
        doc.put("content", sanitizeContent(chunk.getContent()));
        doc.put("embedding", chunk.getEmbedding());
        doc.put("indexedAt", java.time.Instant.now().toString()); // ISO ë¬¸ìì—´
        
        // PRD metadata êµ¬ì¡°
        Map<String, Object> metadata = new HashMap<>();
        metadata.put("title", chunk.getTitle());
        metadata.put("hierarchyLevel", chunk.getHierarchyLevel());
        metadata.put("sequenceInDocument", 0); // ê¸°ë³¸ê°’
        metadata.put("language", "ko"); // í•œêµ­ì–´ ê¸°ë³¸ê°’

        // ì‹¤ì œ íŒŒì¼ íƒ€ì…ì„ SourceDocumentì—ì„œ ì¡°íšŒí•˜ì—¬ ë°˜ì˜ (ê¸°ë³¸ê°’ í•˜ë“œì½”ë”© ì œê±°)
        String resolvedFileType = "UNKNOWN";
        try {
            UUID srcId = UUID.fromString(chunk.getDocumentId());
            resolvedFileType = sourceDocumentRepository.findById(srcId)
                    .map(SourceDocument::getFileType)
                    .orElse("UNKNOWN");
        } catch (Exception e) {
            log.warn("Failed to resolve fileType for documentId={}, defaulting to UNKNOWN", chunk.getDocumentId());
        }
        metadata.put("fileType", resolvedFileType);
        
        // breadcrumbs ì²˜ë¦¬ (ê¸°ë³¸ê°’: ë¹ˆ ë°°ì—´)
        metadata.put("breadcrumbs", Arrays.asList()); // ë¹ˆ ë°°ì—´ ê¸°ë³¸ê°’
        
        doc.put("metadata", metadata);
        
        return doc;
    }

    /**
     * content í•„ë“œì˜ Java ì½”ë“œë‚˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì •ë¦¬í•˜ì—¬ JSON íŒŒì‹± ì—ëŸ¬ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
     */
    private String sanitizeContent(String content) {
        if (content == null || content.trim().isEmpty()) {
            return "";
        }
        
        return content
            // Java ì½”ë“œ ê´€ë ¨ ì •ë¦¬
            .replaceAll("\\{[^}]*\\}", "")           // ì¤‘ê´„í˜¸ ë‚´ìš© ì œê±°
            .replaceAll("\\[.*?\\]", "")             // ëŒ€ê´„í˜¸ ë‚´ìš© ì œê±°
            .replaceAll("\\b(int|String|void|public|private|static|final|class|interface|extends|implements)\\b", "")  // Java í‚¤ì›Œë“œ ì œê±°
            .replaceAll("\\b(if|else|for|while|switch|case|break|continue|return|throw|try|catch|finally)\\b", "")  // Java ì œì–´ë¬¸ ì œê±°
            .replaceAll("\\b(new|this|super|null|true|false)\\b", "")  // Java ë¦¬í„°ëŸ´ ì œê±°
            
            // íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
            .replaceAll("\\s+", " ")                 // ì—°ì† ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
            .replaceAll("[\\r\\n\\t]+", " ")         // ì¤„ë°”ê¿ˆ, íƒ­ì„ ê³µë°±ìœ¼ë¡œ
            .replaceAll("\\s*[;=+\\-*/%<>!&|^~]\\s*", " ")  // ì—°ì‚°ì ì£¼ë³€ ê³µë°± ì •ë¦¬
            
            // ê¸°íƒ€ ì •ë¦¬
            .replaceAll("\\s+", " ")                 // ë‹¤ì‹œ ì—°ì† ê³µë°± ì •ë¦¬
            .trim();
    }

}
